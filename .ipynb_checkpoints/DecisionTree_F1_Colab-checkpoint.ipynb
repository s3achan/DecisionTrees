{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c43a2a",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Decision Trees (F1 PodiumFinish) â€” End-to-End Notebook (Gini vs Entropy)\n",
    "\n",
    "This Colab notebook builds a **Decision Tree classifier from start to finish** using a small, interpretable **Formula 1 (F1) PodiumFinish** dataset.\n",
    "\n",
    "You will:\n",
    "- Create a custom categorical dataset\n",
    "- Oneâ€‘hot encode features\n",
    "- Train **two trees**:\n",
    "  - **CART (Gini impurity)**: `criterion='gini'`\n",
    "  - **Information Gain (Entropy)**: `criterion='entropy'`\n",
    "- **Compare tree splits** visually\n",
    "- Compare **feature importance** with plots\n",
    "- (Optional) Compute **entropy** and **information gain** manually\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6178c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running in Colab, you already have most packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6334b",
   "metadata": {},
   "source": [
    "## 1) Create a small F1 dataset (categorical, interpretable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Weather': [\n",
    "        'Wet','Wet','Wet','Wet','Wet',\n",
    "        'Dry','Dry','Dry','Dry','Wet'\n",
    "    ],\n",
    "    'Starting_Position': [\n",
    "        'Front','Mid','Front','Mid','Back',\n",
    "        'Front','Mid','Back','Front','Back'\n",
    "    ],\n",
    "    'Pit_Strategy': [\n",
    "        'OneStop','OneStop','TwoStop','TwoStop','OneStop',\n",
    "        'OneStop','TwoStop','OneStop','TwoStop','TwoStop'\n",
    "    ],\n",
    "    'Safety_Car': [\n",
    "        'Yes','Yes','Yes','Yes','Yes',\n",
    "        'No','No','No','No','No'\n",
    "    ],\n",
    "    'PodiumFinish': [\n",
    "        'Yes','Yes','Yes','Yes','No',   # Wet + SafetyCar=Yes â†’ Front/Mid = Yes, Back = No\n",
    "        'Yes','Yes','No','Yes','No'    # Dry / no safety car mostly follows starting position\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bc5fd",
   "metadata": {},
   "source": [
    "### Quick sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f46f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nValue counts for target:\")\n",
    "print(df['PodiumFinish'].value_counts())\n",
    "print(\"\\nUnique values per feature:\")\n",
    "print(df.drop(columns=['PodiumFinish']).nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8608659",
   "metadata": {},
   "source": [
    "## 2) Oneâ€‘hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['Weather', 'Starting_Position', 'Pit_Strategy', 'Safety_Car'],\n",
    "    drop_first=False\n",
    ")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2b385",
   "metadata": {},
   "source": [
    "## 3) Split into features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517818d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns=['PodiumFinish'])\n",
    "y = df_encoded['PodiumFinish']\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a894a",
   "metadata": {},
   "source": [
    "## 4) Train Decision Trees (Gini vs Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf600072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (small dataset; this is mainly for demo)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "dt_gini = DecisionTreeClassifier(criterion='gini', random_state=42, max_depth=None)\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42, max_depth=None)\n",
    "\n",
    "dt_gini.fit(X_train, y_train)\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "print(\"Trained dt_gini and dt_entropy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aaa6bf",
   "metadata": {},
   "source": [
    "## 5) Evaluate (quick check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9488d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "eval_model(dt_gini, \"Decision Tree (Gini)\")\n",
    "eval_model(dt_entropy, \"Decision Tree (Entropy / Information Gain)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d4bc3",
   "metadata": {},
   "source": [
    "## 6) Compare tree splits with plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e78cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both trees using the same max_depth so you can compare splits fairly\n",
    "max_depth_to_show = 5\n",
    "\n",
    "plt.figure(figsize=(26, 12))\n",
    "plot_tree(\n",
    "    dt_gini,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['No','Yes'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8,\n",
    "    max_depth=max_depth_to_show\n",
    ")\n",
    "plt.title(f\"Gini Tree (criterion='gini') â€” max_depth={max_depth_to_show}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(26, 12))\n",
    "plot_tree(\n",
    "    dt_entropy,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['No','Yes'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8,\n",
    "    max_depth=max_depth_to_show\n",
    ")\n",
    "plt.title(f\"Entropy Tree (criterion='entropy') â€” max_depth={max_depth_to_show}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6b639",
   "metadata": {},
   "source": [
    "## 7) Compare feature importance (plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Gini_Importance\": dt_gini.feature_importances_,\n",
    "    \"Entropy_Importance\": dt_entropy.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by Gini importance for readability\n",
    "importance_df_sorted = importance_df.sort_values(\"Gini_Importance\", ascending=False)\n",
    "importance_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Gini importances\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(importance_df_sorted[\"Feature\"], importance_df_sorted[\"Gini_Importance\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature Importance â€” Gini\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart: Entropy importances\n",
    "importance_df_sorted2 = importance_df.sort_values(\"Entropy_Importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(importance_df_sorted2[\"Feature\"], importance_df_sorted2[\"Entropy_Importance\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature Importance â€” Entropy (Information Gain)\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6350a",
   "metadata": {},
   "source": [
    "## 8) (Optional) Manual Entropy & Information Gain (for learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y_series):\n",
    "    values, counts = np.unique(y_series, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return -np.sum([p * np.log2(p) for p in probs if p > 0])\n",
    "\n",
    "def information_gain(df_raw, feature, target):\n",
    "    parent_entropy = entropy(df_raw[target])\n",
    "    total = len(df_raw)\n",
    "    weighted_child_entropy = 0.0\n",
    "\n",
    "    for _, subset in df_raw.groupby(feature):\n",
    "        weight = len(subset) / total\n",
    "        weighted_child_entropy += weight * entropy(subset[target])\n",
    "\n",
    "    return parent_entropy - weighted_child_entropy\n",
    "\n",
    "target = \"PodiumFinish\"\n",
    "features = [c for c in df.columns if c != target]\n",
    "\n",
    "root_entropy = entropy(df[target])\n",
    "print(\"Root Entropy:\", round(root_entropy, 4))\n",
    "\n",
    "ig_table = []\n",
    "for f in features:\n",
    "    ig_table.append((f, information_gain(df, f, target)))\n",
    "\n",
    "ig_df = pd.DataFrame(ig_table, columns=[\"Feature\", \"Information_Gain\"]).sort_values(\"Information_Gain\", ascending=False)\n",
    "ig_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613bca9",
   "metadata": {},
   "source": [
    "## 9) Notes\n",
    "\n",
    "- With small datasets, trees can become **very confident** quickly.\n",
    "- `criterion='gini'` and `criterion='entropy'` often pick similar early splits, but can diverge deeper in the tree.\n",
    "- If you want a deeper / more complex tree, increase dataset size or add controlled noise."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DecisionTree_F1_PodiumFinish.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
